{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engrl\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips of working of evolutionary_search library\n",
    "1. It did not work. Started working normally when I installed another library i.e. Neuro_evolution which is keras based and work for holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
      "X contains 26 input features only and there is no output label (1040, 26)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[[1.809000e+00 1.485100e-04 6.800000e-01 8.430000e-01 2.040000e+00\n",
      "  7.881000e+00 7.820000e-01 2.690000e+00 4.543000e+00 1.107300e+01\n",
      "  8.069000e+00 9.255540e-01 9.748100e-02 1.347200e+01 1.192600e+02\n",
      "  1.216300e+02 8.028000e+00 1.081440e+02 1.375460e+02 6.200000e+01\n",
      "  6.000000e+01 8.211245e-03 5.658130e-04 1.818200e+01 1.000000e+00\n",
      "  3.387000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Importing the Train Data \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"E:\\PD Dataset From Laptop/train_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_train = numpy.delete(X_first, 27, 1)\n",
    "X_withlabels = numpy.delete(X_train, 0,1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
    "X = numpy.delete(X_withlabels, 26, 1)\n",
    "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
    "Y = X_withlabels[: ,26]\n",
    "print(Y)\n",
    "print(X[[1039]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below code extracts vowel u phonations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "indices = 2 #if you change this to 1 or 2, also change the increment to 27 and 28, respectively\n",
    "increment=28\n",
    "while i<=39:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "#print(indices)\n",
    "X_train_withlabels = X_withlabels[[indices]]\n",
    "print(X_train_withlabels.shape)\n",
    "Y = X_train_withlabels[:, 26]\n",
    "X = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below Code is for Genetic Algorithm Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import itertools\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "X_Origionl = X\n",
    "y_Origionl = Y\n",
    "\n",
    "x = range(1,101)\n",
    "lda = LDA(n_components=1)\n",
    "X = lda.fit_transform(X_Origionl, y_Origionl)\n",
    "y = y_Origionl\n",
    "paramgrid = {\"hidden_layer_sizes\": list(itertools.combinations(x,2)),\n",
    "             \"solver\" :['lbfgs']\n",
    "            \n",
    "             }\n",
    "print(\"Size: \", len(paramgrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types [1, 1] and maxint [4949, 0] detected\n",
      "--- Evolve in 4950 possible combinations ---\n",
      "gen\tnevals\tavg     \tmin\tmax \tstd      \n",
      "0  \t15    \t0.946667\t0.9\t0.95\t0.0124722\n",
      "1  \t9     \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "2  \t12    \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "3  \t6     \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "4  \t10    \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "5  \t11    \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "6  \t7     \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "7  \t8     \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "8  \t11    \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "9  \t5     \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "10 \t9     \t0.95    \t0.95\t0.95\t3.33067e-16\n",
      "Best individual is: {'hidden_layer_sizes': (12, 79), 'solver': 'lbfgs'}\n",
      "with fitness: 0.95\n",
      "Wall time: 9 s\n",
      "Time Elapsed = 8.995530605316162\n"
     ]
    }
   ],
   "source": [
    "time_a = time.time()\n",
    "if __name__==\"__main__\":\n",
    "    #pool = Pool(4)\n",
    "    cv = EvolutionaryAlgorithmSearchCV(estimator=MLPClassifier(),\n",
    "                                       params=paramgrid,\n",
    "                                       scoring=\"accuracy\",\n",
    "                                       cv=KFold(n_splits=40),\n",
    "                                       verbose=True,\n",
    "                                       population_size=15,\n",
    "                                       gene_mutation_prob=0.10,\n",
    "                                       tournament_size=3,\n",
    "                                       generations_number=10)\n",
    "                                       #pmap = pool.map)\n",
    "    %time cv.fit(X, y)\n",
    "time_b = time.time()\n",
    "print(\"Time Elapsed =\", time_b-time_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time complexity analysis with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LDA Selected Features =================== (40, 1)\n",
      "Best Acc ==================================== 95.0\n",
      "Nodes1 = 1 Nodes2= 4\n",
      "Best Acc ==================================== 97.5\n",
      "Nodes1 = 1 Nodes2= 13\n",
      "Time Elapsed = 5149.544309139252\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_new = lda.fit_transform(X, Y)\n",
    "\n",
    "Best_Acc =0\n",
    "\n",
    "time_a = time.time()\n",
    "Nodes = range(1,101)\n",
    "for nodes1 in Nodes:\n",
    "    for nodes2 in Nodes:\n",
    "        i=0\n",
    "        Net_Acc=0\n",
    "        while i<=39:\n",
    "\n",
    "            #########Setting Train Test Parts for Each Subject\n",
    "            X_test = X_new[[i]]\n",
    "            X_train = numpy.delete(X_new, i, 0)\n",
    "            Y_train = numpy.delete(Y, i, 0)\n",
    "            Y_test = Y[[i]]\n",
    "\n",
    "\n",
    "            ##############Models Part##############################################\n",
    "            #Model Fiting\n",
    "            # checkpoint\n",
    "            model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(nodes1, nodes2,),random_state=1)\n",
    "            model.fit(X_train, Y_train)\n",
    "            Y_pred = model.predict(X_test)\n",
    "            # evaluate the model\n",
    "            scores = accuracy_score(Y_test, Y_pred)\n",
    "            #print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "\n",
    "            if scores==1:\n",
    "                Net_Acc = Net_Acc+1\n",
    "\n",
    "            i = i+1\n",
    "        Acc = (Net_Acc/40)*100\n",
    "        \n",
    "        if (Acc>Best_Acc):\n",
    "            Best_Acc = Acc\n",
    "            print(\"Best Acc ====================================\", Best_Acc)\n",
    "            print(\"Nodes1 =\", nodes1, \"Nodes2=\", nodes2)\n",
    "time_b = time.time()\n",
    "print(\"Time Elapsed =\", time_b-time_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results of Table III of the Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
      "X contains 26 input features only and there is no output label (1040, 26)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Importing the Train Data \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"E:\\PD Dataset From Laptop/train_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_train = numpy.delete(X_first, 27, 1)\n",
    "X_withlabels = numpy.delete(X_train, 0,1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
    "X = numpy.delete(X_withlabels, 26, 1)\n",
    "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
    "Y = X_withlabels[: ,26]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "[   2   28   54   80  106  132  158  184  210  236  262  288  314  340\n",
      "  366  392  418  444  470  496  522  548  574  600  626  652  678  704\n",
      "  730  756  782  808  834  860  886  912  938  964  990 1016]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Extracting Vowel \"u\" Phonations.\n",
    "i=1\n",
    "indices = 2 #if you change this to 1 and increment to 27, we will get Vowel \"o\" Dataset\n",
    "increment=28\n",
    "while i<=39:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "#print(indices)\n",
    "X_train_withlabels = X_withlabels[[indices]]\n",
    "print(X_train_withlabels.shape)\n",
    "Y = X_train_withlabels[:, 26]\n",
    "X = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(indices)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc ==================================== 95.0\n",
      "Sensitivity = 95.0\n",
      "Specificity = 95.0\n",
      "Mathew Correlation Coefficients MCC= 0.9\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_new = lda.fit_transform(X, Y)\n",
    "\n",
    "Best_Acc =0\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "\n",
    "i=0\n",
    "Net_Acc=0\n",
    "while i<=39:\n",
    "\n",
    "    #########Setting Train Test Parts for Each Subject\n",
    "    X_test = X_new[[i]]\n",
    "    X_train = numpy.delete(X_new, i, 0)\n",
    "    Y_train = numpy.delete(Y, i, 0)\n",
    "    Y_test = Y[[i]]\n",
    "\n",
    "\n",
    "    ##############Models Part##############################################\n",
    "    #Model Fiting\n",
    "    # checkpoint\n",
    "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,),random_state=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    scores = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    if i<=19:\n",
    "        if scores==1:\n",
    "            TP = TP+1 #This is for evaluationg sensitivity\n",
    "    if i>19:\n",
    "        if scores==1:\n",
    "            TN = TN+1  #This is for evaluating specificity\n",
    "    \n",
    "\n",
    "    if scores==1:\n",
    "        Net_Acc = Net_Acc+1 #This is for evaluationg overall accuracy\n",
    "\n",
    "    i = i+1\n",
    "Acc = (Net_Acc/40)*100\n",
    "FN=20-TP\n",
    "FP=20-TN\n",
    "\n",
    "Best_Acc = Acc\n",
    "print(\"Acc ====================================\", Best_Acc)\n",
    "print(\"Sensitivity =\", (TP/(20))*100)\n",
    "print(\"Specificity =\", (TN/(20))*100)\n",
    "print(\"Mathew Correlation Coefficients MCC=\", (TP*TN-FP*FN)/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results of Table IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
      "X contains 26 input features only and there is no output label (1040, 26)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Importing the data of Training Database\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"E:\\PD Dataset From Laptop/train_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_train = numpy.delete(X_first, 27, 1)\n",
    "X_withlabels = numpy.delete(X_train, 0,1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
    "X = numpy.delete(X_withlabels, 26, 1)\n",
    "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
    "Y = X_withlabels[: ,26]\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "[   1   27   53   79  105  131  157  183  209  235  261  287  313  339\n",
      "  365  391  417  443  469  495  521  547  573  599  625  651  677  703\n",
      "  729  755  781  807  833  859  885  911  937  963  989 1015]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Extracting Vowel \"o\" Phonations.\n",
    "i=1\n",
    "indices = 1 #if you change this to 1 and increment to 27, we will get Vowel \"o\" Dataset\n",
    "increment=27\n",
    "while i<=39:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "#print(indices)\n",
    "X_train_withlabels = X_withlabels[[indices]]\n",
    "print(X_train_withlabels.shape)\n",
    "Y = X_train_withlabels[:, 26]\n",
    "X = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(indices)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc ==================================== 92.5\n",
      "Sensitivity = 95.0\n",
      "Specificity = 90.0\n",
      "Mathew Correlation Coefficients MCC= 0.8510644963469901\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_new = lda.fit_transform(X, Y)\n",
    "\n",
    "Best_Acc =0\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "\n",
    "i=0\n",
    "Net_Acc=0\n",
    "while i<=39:\n",
    "\n",
    "    #########Setting Train Test Parts for Each Subject\n",
    "    X_test = X_new[[i]]\n",
    "    X_train = numpy.delete(X_new, i, 0)\n",
    "    Y_train = numpy.delete(Y, i, 0)\n",
    "    Y_test = Y[[i]]\n",
    "\n",
    "\n",
    "    ##############Models Part##############################################\n",
    "    #Model Fiting\n",
    "    # checkpoint\n",
    "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,),random_state=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    scores = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    if i<=19:\n",
    "        if scores==1:\n",
    "            TP = TP+1\n",
    "    if i>19:\n",
    "        if scores==1:\n",
    "            TN = TN+1\n",
    "    \n",
    "\n",
    "    if scores==1:\n",
    "        Net_Acc = Net_Acc+1\n",
    "\n",
    "    i = i+1\n",
    "Acc = (Net_Acc/40)*100\n",
    "FN=20-TP\n",
    "FP=20-TN\n",
    "\n",
    "Best_Acc = Acc\n",
    "print(\"Acc ====================================\", Best_Acc)\n",
    "print(\"Sensitivity =\", (TP/(20))*100)\n",
    "print(\"Specificity =\", (TN/(20))*100)\n",
    "print(\"Mathew Correlation Coefficients MCC=\", (TP*TN-FP*FN)/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results of Table V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
      "X contains 26 input features only and there is no output label (1040, 26)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Importing the data of Training Database\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"E:\\PD Dataset From Laptop/train_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_train = numpy.delete(X_first, 27, 1)\n",
    "X_withlabels = numpy.delete(X_train, 0,1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
    "X = numpy.delete(X_withlabels, 26, 1)\n",
    "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
    "Y = X_withlabels[: ,26]\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "[   0   26   52   78  104  130  156  182  208  234  260  286  312  338\n",
      "  364  390  416  442  468  494  520  546  572  598  624  650  676  702\n",
      "  728  754  780  806  832  858  884  910  936  962  988 1014]\n"
     ]
    }
   ],
   "source": [
    "#Extracting Vowel \"a\" Dataset\n",
    "i=1\n",
    "indices = 0 #if you change this to 1 and increment to 27, we will get Vowel \"o\" Dataset\n",
    "increment=26\n",
    "while i<=39:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "#print(indices)\n",
    "X_train_withlabels = X_withlabels[[indices]]\n",
    "print(X_train_withlabels.shape)\n",
    "Y = X_train_withlabels[:, 26]\n",
    "X = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc ==================================== 92.5\n",
      "Sensitivity = 95.0\n",
      "Specificity = 90.0\n",
      "Mathew Correlation Coefficients MCC= 0.8510644963469901\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_new = lda.fit_transform(X, Y)\n",
    "\n",
    "Best_Acc =0\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "\n",
    "i=0\n",
    "Net_Acc=0\n",
    "while i<=39:\n",
    "\n",
    "    #########Setting Train Test Parts for Each Subject\n",
    "    X_test = X_new[[i]]\n",
    "    X_train = numpy.delete(X_new, i, 0)\n",
    "    Y_train = numpy.delete(Y, i, 0)\n",
    "    Y_test = Y[[i]]\n",
    "\n",
    "\n",
    "    ##############Models Part##############################################\n",
    "    #Model Fiting\n",
    "    # checkpoint\n",
    "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,),random_state=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    scores = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    if i<=19:\n",
    "        if scores==1:\n",
    "            TP = TP+1\n",
    "    if i>19:\n",
    "        if scores==1:\n",
    "            TN = TN+1\n",
    "    \n",
    "\n",
    "    if scores==1:\n",
    "        Net_Acc = Net_Acc+1\n",
    "\n",
    "    i = i+1\n",
    "Acc = (Net_Acc/40)*100\n",
    "FN=20-TP\n",
    "FP=20-TN\n",
    "\n",
    "Best_Acc = Acc\n",
    "print(\"Acc ====================================\", Best_Acc)\n",
    "print(\"Sensitivity =\", (TP/(20))*100)\n",
    "print(\"Specificity =\", (TN/(20))*100)\n",
    "print(\"Mathew Correlation Coefficients MCC=\", (TP*TN-FP*FN)/(math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results of Table VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
      "X contains 26 input features only and there is no output label (1040, 26)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# load pima indians dataset\n",
    "#Importing the Train Data \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"E:\\PD Dataset From Laptop/train_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_train = numpy.delete(X_first, 27, 1)\n",
    "X_withlabels = numpy.delete(X_train, 0,1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
    "X = numpy.delete(X_withlabels, 26, 1)\n",
    "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
    "Y = X_withlabels[: ,26]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,)\n",
      "(80, 27)\n",
      "(80, 26)\n",
      "(80,)\n"
     ]
    }
   ],
   "source": [
    "#Extracting vowel data from training database\n",
    "i=2\n",
    "indices = numpy.array(range(0, 2))\n",
    "lb = 26\n",
    "ub = 28\n",
    "while i<=40:\n",
    "    indices = numpy.append(indices, range(lb, ub))    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    lb = lb+26\n",
    "    ub = ub+26\n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "#print(indices)\n",
    "X_train_withlabels = X_withlabels[[indices]]\n",
    "print(X_train_withlabels.shape)\n",
    "Y_train = X_train_withlabels[:, 26]\n",
    "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (168, 27)\n",
      "(168, 26)\n",
      "(168,)\n"
     ]
    }
   ],
   "source": [
    "#Importing the Test Data \n",
    "df = pd.read_csv(\"E:\\PD Dataset From Laptop/test_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_test_withlabels = numpy.delete(X_first, 0,1)\n",
    "X_test = numpy.delete(X_test_withlabels, 26, 1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_test_withlabels.shape)   \n",
    "print(X_test.shape)\n",
    "Y_test = X_test_withlabels[:, 26]\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_FS_train = lda.fit_transform(X_train, Y_train)\n",
    "\n",
    "\n",
    "######It is important to note below that we are using LDA model fitted on Training Data and Reduce the testing data then.\n",
    "lda = LDA(n_components=1)\n",
    "X_FS_test = lda.fit(X_train, Y_train).transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Acc = 100.0\n"
     ]
    }
   ],
   "source": [
    "########### LOSO on Testing Database##############################\n",
    "\n",
    "Best_Acc=0\n",
    "\n",
    "Net_Acc=0\n",
    "i=1\n",
    "lb=0\n",
    "ub=6\n",
    "while i<=28:\n",
    "\n",
    "    #########Setting Train Test Parts for Each Subject\n",
    "    X_test_subj = X_FS_test[range(lb,ub)]\n",
    "    Y_test_subj = Y_test[range(lb,ub)]\n",
    "    #print(X_test_subj.shape)\n",
    "    #print(Y_test_subj.shape)\n",
    "\n",
    "    ##############Models Part##############################################\n",
    "    #Model Fiting\n",
    "    # checkpoint\n",
    "    ##############Models Part##############################################\n",
    "\n",
    "    model =  MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(2, 29,), random_state=1)\n",
    "    model.fit(X_FS_train, Y_train)\n",
    "    Y_pred = model.predict(X_test_subj)\n",
    "    scores = accuracy_score(Y_test_subj, Y_pred)\n",
    "    if scores>=0.5:\n",
    "        Net_Acc = Net_Acc+1\n",
    "\n",
    "\n",
    "    lb=lb+6\n",
    "    ub=lb+6\n",
    "    i = i+1\n",
    "Acc = (Net_Acc*100)/28\n",
    "\n",
    "Best_Acc = Acc\n",
    "print(\"Best Acc =\", Best_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
