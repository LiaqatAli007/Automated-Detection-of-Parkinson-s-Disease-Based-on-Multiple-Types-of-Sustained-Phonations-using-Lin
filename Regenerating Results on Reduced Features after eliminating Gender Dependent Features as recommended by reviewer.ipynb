{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engrl\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "global Best_Acc\n",
    "Best_Acc=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (1040, 27)\n",
      "X contains 26 input features only and there is no output label (1040, 26)\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Importing the Data of Training Database\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"F:\\Parkinsons Disease PD/train_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_train = numpy.delete(X_first, 27, 1)\n",
    "X_withlabels = numpy.delete(X_train, 0,1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_withlabels.shape)               \n",
    "X = numpy.delete(X_withlabels, 26, 1)\n",
    "print(\"X contains 26 input features only and there is no output label\", X.shape)\n",
    "Y = X_withlabels[: ,26]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[   0   26   52   78  104  130  156  182  208  234  260  286  312  338\n",
      "  364  390  416  442  468  494  520  546  572  598  624  650  676  702\n",
      "  728  754  780  806  832  858  884  910  936  962  988 1014]\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "(40, 15)\n"
     ]
    }
   ],
   "source": [
    "#Extracting Vowel \"a\" phonations.\n",
    "i=2\n",
    "indices = 0\n",
    "increment = 26\n",
    "while i<=40:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "    \n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "print(indices)\n",
    "X_train_withlabels = X_withlabels[indices]\n",
    "print(X_train_withlabels.shape)\n",
    "Y_train = X_train_withlabels[:, 26]\n",
    "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on Training Database using Vowel A using LDA-GA-NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import itertools\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "X_Origionl = X_train\n",
    "y_Origionl = Y_train\n",
    "\n",
    "x = range(1,101)\n",
    "lda = LDA(n_components=1)\n",
    "X = lda.fit_transform(X_Origionl, y_Origionl)\n",
    "y = y_Origionl\n",
    "paramgrid = {\"hidden_layer_sizes\": list(itertools.combinations(x,2)),\n",
    "             \"solver\" :['adam']\n",
    "            \n",
    "             }\n",
    "print(\"Size: \", len(paramgrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types [1, 1] and maxint [0, 4949] detected\n",
      "--- Evolve in 4950 possible combinations ---\n",
      "gen\tnevals\tavg     \tmin  \tmax \tstd      \n",
      "0  \t15    \t0.808333\t0.775\t0.85\t0.0252763\n",
      "1  \t8     \t0.82    \t0.775\t0.85\t0.0208167\n",
      "2  \t7     \t0.826667\t0.8  \t0.85\t0.0143372\n",
      "3  \t7     \t0.838333\t0.825\t0.85\t0.0124722\n",
      "4  \t12    \t0.846667\t0.825\t0.85\t0.00849837\n",
      "5  \t8     \t0.848333\t0.825\t0.85\t0.0062361 \n",
      "6  \t10    \t0.85    \t0.85 \t0.85\t1.11022e-16\n",
      "7  \t10    \t0.85    \t0.85 \t0.85\t1.11022e-16\n",
      "8  \t11    \t0.85    \t0.85 \t0.85\t1.11022e-16\n",
      "9  \t11    \t0.846667\t0.8  \t0.85\t0.0124722  \n",
      "10 \t7     \t0.846667\t0.8  \t0.85\t0.0124722  \n",
      "Best individual is: {'solver': 'adam', 'hidden_layer_sizes': (76, 89)}\n",
      "with fitness: 0.85\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "#time_a = time.time()\n",
    "if __name__==\"__main__\":\n",
    "    #pool = Pool(4)\n",
    "    cv = EvolutionaryAlgorithmSearchCV(estimator=MLPClassifier(),\n",
    "                                       params=paramgrid,\n",
    "                                       scoring=\"accuracy\",\n",
    "                                       cv=KFold(n_splits=40),\n",
    "                                       verbose=True,\n",
    "                                       population_size=15,\n",
    "                                       gene_mutation_prob=0.10,\n",
    "                                       tournament_size=3,\n",
    "                                       generations_number=10)\n",
    "                                       #pmap = pool.map)\n",
    "    %time cv.fit(X, y)\n",
    "#time_b = time.time()\n",
    "#print(\"Time Elapsed =\", time_b-time_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results Reported in Paper (Training Database, Vowel A Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[   0   26   52   78  104  130  156  182  208  234  260  286  312  338\n",
      "  364  390  416  442  468  494  520  546  572  598  624  650  676  702\n",
      "  728  754  780  806  832  858  884  910  936  962  988 1014]\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "(40, 15)\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "indices = 0\n",
    "increment = 26\n",
    "while i<=40:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "    \n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "print(indices)\n",
    "X_train_withlabels = X_withlabels[indices]\n",
    "print(X_train_withlabels.shape)\n",
    "Y_train = X_train_withlabels[:, 26]\n",
    "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_FS = lda.fit_transform(X_train, Y_train)\n",
    "Y_data = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Acc ==================================== 85.0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "cor_class=0\n",
    "LOSO_Acc=0\n",
    "while i<=39:\n",
    "    X_test = X_FS[[i]]\n",
    "    X_train = numpy.delete(X_FS, i, 0)\n",
    "    Y_test = Y_data[[i]]\n",
    "    Y_train = numpy.delete(Y_data, i, 0)\n",
    "    ###############For Each Selected Features, evaluate model performance by trying different nodes############\n",
    "    #model =  MLPClassifier(solver='adam', hidden_layer_sizes=(nodes1,  ), random_state=1)\n",
    "    model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(32, 2, ), random_state=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    if(i<=19):\n",
    "        if predictions==1:\n",
    "            cor_class = cor_class+1\n",
    "    if(i>19):\n",
    "        if predictions==0:\n",
    "            cor_class = cor_class+1\n",
    "\n",
    "    i = i+1\n",
    "LOSO_Acc= (cor_class/40)*100\n",
    "if (LOSO_Acc>=80):\n",
    "    Best_Acc = LOSO_Acc\n",
    "    print(\"Best Acc ====================================\", Best_Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results Reported in Paper (Testing Database, Vowel A Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[   0   26   52   78  104  130  156  182  208  234  260  286  312  338\n",
      "  364  390  416  442  468  494  520  546  572  598  624  650  676  702\n",
      "  728  754  780  806  832  858  884  910  936  962  988 1014]\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "(40, 15)\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "indices = 0\n",
    "increment = 26\n",
    "while i<=40:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "    \n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "print(indices)\n",
    "X_train_withlabels = X_withlabels[indices]\n",
    "print(X_train_withlabels.shape)\n",
    "Y_train = X_train_withlabels[:, 26]\n",
    "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (168, 27)\n",
      "(168, 26)\n",
      "(168,)\n"
     ]
    }
   ],
   "source": [
    "#Importing the Test Data \n",
    "df = pd.read_csv(\"F:\\Parkinsons Disease PD/test_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_test_withlabels = numpy.delete(X_first, 0,1)\n",
    "X_test = numpy.delete(X_test_withlabels, 26, 1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_test_withlabels.shape)   \n",
    "print(X_test.shape)\n",
    "Y_test = X_test_withlabels[:, 26]\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 15)\n",
      "(168, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "lda = LDA(n_components=1)\n",
    "X_FS_train = lda.fit_transform(X_train, Y_train)\n",
    "\n",
    "\n",
    "X_test = numpy.delete(X_test, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_test.shape)\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "X_FS_test = lda.fit(X_train, Y_train).transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#lda = LDA(n_components=1)\n",
    "#X_FS_test = lda.fit_transform(X_test, Y_test)\n",
    "#print(X_FS_test_usingtrain-X_FS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Acc = 64.28571428571429\n"
     ]
    }
   ],
   "source": [
    "Net_Acc=0\n",
    "i=1\n",
    "lb=0 #Considering Vowel a only\n",
    "ub=3\n",
    "while i<=28:\n",
    "\n",
    "    #########Setting Train Test Parts for Each Subject\n",
    "    X_test_subj = X_FS_test[range(lb,ub)]\n",
    "    Y_test_subj = Y_test[range(lb,ub)]\n",
    "    #print(X_test_subj.shape)\n",
    "    #print(Y_test_subj.shape)\n",
    "\n",
    "    ##############Models Part##############################################\n",
    "    #Model Fiting\n",
    "    # checkpoint\n",
    "    ##############Models Part##############################################\n",
    "\n",
    "    model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(32, 2, ), random_state=1)\n",
    "    model.fit(X_FS_train, Y_train)\n",
    "    Y_pred = model.predict(X_test_subj)\n",
    "    scores = accuracy_score(Y_test_subj, Y_pred)\n",
    "    if scores>0.5:\n",
    "        Net_Acc = Net_Acc+1\n",
    "\n",
    "\n",
    "    lb=lb+6\n",
    "    ub=lb+3\n",
    "    i = i+1\n",
    "Acc = (Net_Acc*100)/28\n",
    "#if (Acc>=60):\n",
    "    #Best_Acc = Acc\n",
    "print(\"Best Acc =\", Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results for Vowel O Dataset (Considering Training Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[   1   27   53   79  105  131  157  183  209  235  261  287  313  339\n",
      "  365  391  417  443  469  495  521  547  573  599  625  651  677  703\n",
      "  729  755  781  807  833  859  885  911  937  963  989 1015]\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "(40, 15)\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "indices = 1\n",
    "increment = 27\n",
    "while i<=40:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "    \n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "print(indices)\n",
    "X_train_withlabels = X_withlabels[indices]\n",
    "print(X_train_withlabels.shape)\n",
    "Y_train = X_train_withlabels[:, 26]\n",
    "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA(n_components=1)\n",
    "X_FS = lda.fit_transform(X_train, Y_train)\n",
    "Y_data = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Acc ==================================== 80.0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "cor_class=0\n",
    "LOSO_Acc=0\n",
    "while i<=39:\n",
    "    X_test = X_FS[[i]]\n",
    "    X_train = numpy.delete(X_FS, i, 0)\n",
    "    Y_test = Y_data[[i]]\n",
    "    Y_train = numpy.delete(Y_data, i, 0)\n",
    "    ###############For Each Selected Features, evaluate model performance by trying different nodes############\n",
    "    #model =  MLPClassifier(solver='adam', hidden_layer_sizes=(nodes1,  ), random_state=1)\n",
    "    model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(6, 7, ), random_state=1)\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    if(i<=19):\n",
    "        if predictions==1:\n",
    "            cor_class = cor_class+1\n",
    "    if(i>19):\n",
    "        if predictions==0:\n",
    "            cor_class = cor_class+1\n",
    "\n",
    "    i = i+1\n",
    "LOSO_Acc= (cor_class/40)*100\n",
    "if (LOSO_Acc>=80):\n",
    "    Best_Acc = LOSO_Acc\n",
    "    print(\"Best Acc ====================================\", Best_Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerating Results Reported in Paper (Testing Database, Vowel o Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n",
      "[   1   27   53   79  105  131  157  183  209  235  261  287  313  339\n",
      "  365  391  417  443  469  495  521  547  573  599  625  651  677  703\n",
      "  729  755  781  807  833  859  885  911  937  963  989 1015]\n",
      "(40, 27)\n",
      "(40, 26)\n",
      "(40,)\n",
      "(40, 15)\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "indices = 1\n",
    "increment = 27\n",
    "while i<=40:\n",
    "    indices = numpy.append(indices, increment)    #Indices will hold all indices of ist three samples in each subject\n",
    "    i = i+1\n",
    "    increment = increment+26\n",
    "    \n",
    "print(indices.shape)         #Thus we get 40x3=120 indices of 120samples. Now extract those samples from X and Y to get a new Training Set\n",
    "print(indices)\n",
    "X_train_withlabels = X_withlabels[indices]\n",
    "print(X_train_withlabels.shape)\n",
    "Y_train = X_train_withlabels[:, 26]\n",
    "X_train = numpy.delete(X_train_withlabels, 26, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_train = numpy.delete(X_train, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with contains 26 input features and class label but not with patient id (168, 27)\n",
      "(168, 26)\n",
      "(168,)\n"
     ]
    }
   ],
   "source": [
    "#Importing the Test Data \n",
    "df = pd.read_csv(\"F:\\Parkinsons Disease PD/test_data.txt\")\n",
    "X_first = numpy.array(df)\n",
    "X_test_withlabels = numpy.delete(X_first, 0,1)\n",
    "X_test = numpy.delete(X_test_withlabels, 26, 1)\n",
    "print(\"X_with contains 26 input features and class label but not with patient id\", X_test_withlabels.shape)   \n",
    "print(X_test.shape)\n",
    "Y_test = X_test_withlabels[:, 26]\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 15)\n",
      "(168, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "lda = LDA(n_components=1)\n",
    "X_FS_train = lda.fit_transform(X_train, Y_train)\n",
    "\n",
    "\n",
    "X_test = numpy.delete(X_test, [0, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22], 1) #Removed features recommended by reviewer\n",
    "print(X_test.shape)\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "X_FS_test = lda.fit(X_train, Y_train).transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Acc = 82.14285714285714\n"
     ]
    }
   ],
   "source": [
    "Net_Acc=0\n",
    "i=1\n",
    "lb=3   ####This command is different from that of Data a. \n",
    "ub=6\n",
    "while i<=28:\n",
    "\n",
    "    #########Setting Train Test Parts for Each Subject\n",
    "    X_test_subj = X_FS_test[range(lb,ub)]\n",
    "    Y_test_subj = Y_test[range(lb,ub)]\n",
    "    #print(X_test_subj.shape)\n",
    "    #print(Y_test_subj.shape)\n",
    "\n",
    "    ##############Models Part##############################################\n",
    "    #Model Fiting\n",
    "    # checkpoint\n",
    "    ##############Models Part##############################################\n",
    "\n",
    "    #model =  MLPClassifier(solver='adam',  hidden_layer_sizes=(nodes1, nodes2, ), random_state=1)\n",
    "    model =  MLPClassifier(solver='adam', hidden_layer_sizes=(6, 7, ), random_state=1)\n",
    "    model.fit(X_FS_train, Y_train)\n",
    "    Y_pred = model.predict(X_test_subj)\n",
    "    scores = accuracy_score(Y_test_subj, Y_pred)\n",
    "    if scores>0.5:\n",
    "        Net_Acc = Net_Acc+1\n",
    "\n",
    "\n",
    "    lb=lb+6\n",
    "    ub=lb+3\n",
    "    i = i+1\n",
    "Acc = (Net_Acc*100)/28\n",
    "print(\"Best Acc =\", Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
